{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1>Geleneksel Doğal Dil İşleme Yöntemleri ve Uygulamaları</h1> </center>\n",
    "\n",
    "<a class = 'anchor' id = '0.'></a><h2>İçindekiler</h2>\n",
    "<font size = 3>\n",
    "* [1. Çalışmanın Özeti](#1.)\n",
    "* [2. Cümlenden İsim Sözcüklerini Çıkarma](#2.)\n",
    "* [3. Metin Benzerliği Hesaplama](#3.)\n",
    "* * [3.1. Kosine Benzerlik Hesaplama](#3.1.)\n",
    "* * [3.2. Phonetic Benzerlik Eşleşme](#3.2.)\n",
    "* [4. Cümledeki Sözcükelerin Etiketlenmesi](#4.)\n",
    "* [5. Cümledeki Sözcüklerden Varlıkları Çıkarma (Named Entity Recognition)](#5.)\n",
    "* [6. Başlık Belirleme](#6.)\n",
    "* [7. Metin Sınıflandırma](#7.)\n",
    "* [8. Duygu Analizi (TextBlob Kütüphanesiyle)](#8.)\n",
    "* [9. Belirsizlik Giderme](#9.)\n",
    "* [10. Konuşmayı Metne Dönüştürme](#10.)\n",
    "* [11. Metni Konuşmaya Dönüştürme](#11.)\n",
    "* [12. Tercüme Yapma](#12.)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[İçindekiler Menüsüne Git](#0.)\n",
    "# <a class='anchor' id ='1.'></a>1. Çalışmanın Özeti\n",
    "\n",
    "Bu çalışmada geleneksel doğal dil işleme yöntemleri ve bu yöntemler kullanılarak uygulamalar geliştirilecektir. Üzerinde çalışılacak yöntemler aşagıdaki gibidir:\n",
    "\n",
    "    * Cümleden isim çıkarma\n",
    "    * Metin benzerliği\n",
    "    * Cümledeki sözcüklerin etiketlenmesi\n",
    "    * Named Entitiy Recognition\n",
    "    * Başlık modelleme\n",
    "    * Metin sınıflandırma\n",
    "    * Duygu analizi\n",
    "    * Kelime anlam karmaşıklığı\n",
    "    * Konuşma tanıma ve konuşmadan metne dönüşüm\n",
    "    * Metinden konuşmaya\n",
    "    * Dil tespiti ve tercübe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[İçindekiler Menüsüne Git](#0.)\n",
    "# <a class='anchor' id ='2.'></a>2. Cümlenden İsim Sözcüklerini Çıkarma\n",
    "\n",
    "Cümleden isim sözcüklerinin çıkarmak için textblob kütüphanesinden TextBlob nesnesi kullanılacaktır. TextBlob nesnesine sözcük girdi olarak verilecek ve oluşturulan nesnenin <b>noun_phareses </b> özelliğinden isim sözcükleri alılacaktır.\n",
    "\n",
    "Uyarı : Bu özellik Türkçe için bu kütüphanede yoktur. Hangi kütüphanelerde Türkçe cümleden isim çıkarma özelliği olduğu araştırılacaktır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob('Serkan is working on natural language processing subjects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serkan\n",
      "natural language processing subjects\n"
     ]
    }
   ],
   "source": [
    "for np in blob.noun_phrases:\n",
    "    print(np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[İçindekiler Menüsüne Git](#0.)\n",
    "# <a class='anchor' id ='3.'></a> 3. Metin Benzerliği Hesaplama\n",
    "\n",
    "İki metin arasındaki benzerliği bulmak çoğu zaman ihtiyaç duyulan bir hesaplamadır. İki metin arasındaki benzerliği bulmak için farklı hesaplama metrikleri vardır:\n",
    "\n",
    "* <b> Cosine benzerliği:</b> İki vektör arasındaki cosine açısını hesaplar.\n",
    "* <b> Jaccard benzerliği: </b> İki metin arasındaki ortak kelimelerin toplam kelimeye bölümüyle elde edilir.\n",
    "* <b> Levenshtein benzerliği :</b> A dizgisini B dizgisine dönüştürmek için gerekli minumum değişiklik, ekleme, düzeltmeyi hesaplar\n",
    "* <b> Hamming bezerliği:</b> Aynı uzunluğa sahip iki dizge arasındaki aynı pozisyondaki sembol farklılıklarını hesaplar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[İçindekiler Menüsüne Git](#0.)\n",
    "## <a class='anchor' id ='3.1.'></a> 3.1. Kosine Benzerlik Hesaplama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorular_tfidf.shape: (5, 33)\n"
     ]
    }
   ],
   "source": [
    "sorular = ['İzmir’de kentsel dönüşüm nerelerde yapılmaktadır?',\n",
    "          'İzmir Büyükşehir Belediyesinin süt soğutma tankı desteklerinden nasıl faydalanabilirim?',\n",
    "          'Spor okulları hakkında nasıl bilgi alabilirim?',\n",
    "          'Haşere ilaçlaması için nereye başvurmalıyım?',\n",
    "          'Bakamayacağım köpeğimi belediyeye ait bir barınağa teslim edebilir miyim?']\n",
    "\n",
    "tfidfVectorizer = TfidfVectorizer()\n",
    "tfidfVectorizer.fit(sorular)\n",
    "sorular_tfidf = tfidfVectorizer.transform(sorular)\n",
    "print('sorular_tfidf.shape:', sorular_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.40075818 0.         0.25819889 0.        ]\n",
      " [0.         0.         0.2428734  0.51639778 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "yeni_soru = ['belediye süt bankası soğutma için', 'ilaçlaması hakkında nereye']\n",
    "yeni_soru_tfidf = tfidfVectorizer.transform(yeni_soru)\n",
    "print(cosine_similarity(yeni_soru_tfidf, sorular_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[İçindekiler Menüsüne Git](#0.)\n",
    "## <a class='anchor' id ='3.2.'></a> 3.2. Phonetic Benzerlik Eşleşme\n",
    "\n",
    "Phonetic eşleşme yazımı benzer kelimeleri ortak kodlarla temsil eden benzerlik eşleştirme yöntemidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuzzy yükleme başarısız olduğu için kodları yazmakla yetiniyorum\n",
    "# import fuzzy\n",
    "#soundex = fuzzy.Soundex()\n",
    "#soundex('natural') #N364 kodunu alacaktır\n",
    "#soundex('natuaral') # N364 kodunu alacaktır"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[İçindekiler Menüsüne Git](#0.)\n",
    "# <a class='anchor' id ='4.'></a>4. Cümledeki Sözcüklerin Etiketlenmesi (Tagging Part of Speech (POS))\n",
    "\n",
    "POS; Varlık Tanıma (Named Entity Recognition), Soru/Cevaplama (Question/Answering), Kelime Anlam Belirsizliği (Word Sense Disambiguation) ve Duygu Analizi gibi doğal dil işleme problemlerinin çözümünün temellerini oluşturur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learnign', 'natural', 'language', 'processing', 'with', 'reading', 'Python', 'book', 'written', 'by', 'Akshay']\n",
      "['I', 'learnign', 'natural', 'language', 'processing', 'reading', 'Python', 'book', 'written', 'Akshay']\n",
      "[('I', 'PRP'), ('learnign', 'VBP'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('reading', 'VBG'), ('Python', 'NNP'), ('book', 'NN'), ('written', 'VBN'), ('Akshay', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "stopwords_eng = set(stopwords.words('english'))\n",
    "\n",
    "text = 'I am learnign natural language processing with reading Python book written by Akshay'\n",
    "\n",
    "word_token = word_tokenize(text)\n",
    "print(word_token)\n",
    "word_token = [word for word in word_token if not word in stopwords_eng]\n",
    "print(word_token)\n",
    "pos_tag = nltk.pos_tag(word_token)\n",
    "print(pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[İçindekiler Menüsüne Git](#0.)\n",
    "# <a class='anchor' id ='5.'></a>5. Cümledeki Sözcüklerden Varlıkları Çıkarma (Named Entity Recognition)\n",
    "\n",
    "NER metin içerisinde yer alan Adres, Kişi, Organizasyon gibi varlıkları çıkarmaya yarayan doğal dil işleme görevidir.Bunun için nltk kütüphanesinde yer alan ne_chunk fonksiyonu kullanılacaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON John/NNP)\n",
      "  is/VBZ\n",
      "  studying/VBG\n",
      "  at/IN\n",
      "  (ORGANIZATION Stanford/NNP University/NNP)\n",
      "  in/IN\n",
      "  (GPE California/NNP))\n"
     ]
    }
   ],
   "source": [
    "text = 'John is studying at Stanford University in California'\n",
    "word_token = word_tokenize(text)\n",
    "pos_tag = nltk.pos_tag(word_token)\n",
    "ner  = nltk.ne_chunk(pos_tag, binary = False)\n",
    "print(ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[İçindekiler Menüsüne Git](#0.)\n",
    "# <a class='anchor' id ='6.'></a>6. Başlık Belirleme\n",
    "\n",
    "Başlık belirleme dökümanları konularına göre ayırmak için kullanılan doğal dil işleme görevidir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ohh I belive car amazing'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self, processors):\n",
    "        self.processors = processors\n",
    "    \n",
    "    def process(self, text):\n",
    "        for processor in self.processors:\n",
    "            text = processor.process(text)\n",
    "        return text\n",
    "\n",
    "class Lower:\n",
    "    def process(self, text):\n",
    "        return text.lower()\n",
    "    \n",
    "class FilterPunctuation:\n",
    "    def __init__(self):\n",
    "        self.punctuations = string.punctuation\n",
    "    def process(self, text):\n",
    "        return ''.join([c for c in text if c not in self.punctuations])\n",
    "\n",
    "class FilterStopWords:\n",
    "    def __init__(self, lang):\n",
    "        self.stopwords = stopwords.words(lang)\n",
    "    def process(self, text):\n",
    "        return ' '.join([word for word in text.split() if not word in self.stopwords])\n",
    "class Lemmatize:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    \n",
    "    def process(self, text):\n",
    "        return ' '.join([self.lemmatizer.lemmatize(word) for word in text.split()])\n",
    "text = 'Ohh! I can not belive that!@ cars# is amazing!'\n",
    "\n",
    "processors = [FilterPunctuation(), \n",
    "              FilterStopWords(lang = 'english'),\n",
    "             Lemmatize()]\n",
    "textProcessor = TextProcessor(processors = processors)\n",
    "textProcessor.process(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'learning', 'NLP', 'interesting', 'exciting', 'includes', 'machine', 'learning', 'staff', 'deep', 'learning', 'reciepts'], ['My', 'father', 'data', 'scientist', 'nlp', 'expert'], ['My', 'sister', 'good', 'exposure', 'android', 'development']]\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"I am learning NLP, it is very interesting and exciting. it includes machine learning staffs and deep learning reciepts\"\n",
    "doc2 = \"My father is a data scientist and he is nlp expert\"\n",
    "doc3 = \"My sister has goods exposure into android development\"\n",
    "docs = [doc1, doc2, doc3]\n",
    "docs_clean = [textProcessor.process(text).split() for text in docs]\n",
    "print(docs_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 3), (7, 1), (8, 1), (9, 1)], [(10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)], [(10, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1)]]\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(docs_clean)\n",
    "doc_term_matrix  = [dictionary.doc2bow(doc) for doc in docs_clean]\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.048*\"good\" + 0.048*\"exposure\" + 0.048*\"development\" + 0.048*\"android\" + 0.048*\"scientist\" + 0.048*\"nlp\" + 0.048*\"data\" + 0.048*\"sister\" + 0.048*\"father\" + 0.048*\"expert\"'), (1, '0.123*\"My\" + 0.070*\"data\" + 0.070*\"nlp\" + 0.070*\"father\" + 0.070*\"expert\" + 0.070*\"sister\" + 0.070*\"development\" + 0.070*\"exposure\" + 0.070*\"android\" + 0.070*\"scientist\"'), (2, '0.175*\"learning\" + 0.070*\"NLP\" + 0.070*\"deep\" + 0.070*\"exciting\" + 0.070*\"includes\" + 0.070*\"interesting\" + 0.070*\"I\" + 0.070*\"machine\" + 0.070*\"reciepts\" + 0.070*\"staff\"')]\n"
     ]
    }
   ],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics = 3, id2word = dictionary, passes = 50)\n",
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[İçindekiler Menüsüne Git](#0.)\n",
    "# <a class='anchor' id ='7.'></a> 7. Metin Sınıflandırma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: ['ham' 'spam']\n",
      "Eğitim başarı oranı:99.56\n",
      "Test başarı oranı:97.91\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porterStemmer = PorterStemmer()\n",
    "class WordStemmer:\n",
    "    def ___init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "    \n",
    "    def process(self, text):\n",
    "        stemmer = PorterStemmer()\n",
    "        return ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "dataset = pd.read_csv('datasets/spam.csv', encoding = 'latin1')\n",
    "dataset = dataset[['v2', 'v1']]\n",
    "dataset.columns = ['email', 'target']\n",
    "\n",
    "X = dataset['email'].values\n",
    "y = dataset['target'].values\n",
    "\n",
    "processors = [Lower(),\n",
    "              FilterPunctuation(), \n",
    "              FilterStopWords(lang = 'english'),\n",
    "             Lemmatize()]\n",
    "\n",
    "textProcessor = TextProcessor(processors)\n",
    "X = [textProcessor.process(x) for x in X]\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "classes = le.inverse_transform(np.arange(len(np.unique(y))))\n",
    "print('classes:', classes)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(X_train)\n",
    "X_train_tfidf = tfidf.transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "clf = MultinomialNB(alpha = 0.2)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "print('Eğitim başarı oranı:{:.2f}'.format(100*clf.score(X_train_tfidf, y_train)))\n",
    "print('Test başarı oranı:{:.2f}'.format(100*clf.score(X_test_tfidf, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[İçindekiler Menüsüne Git](#0.)\n",
    "# <a class='anchor' id ='8.'></a> 8. Duygu Analizi (TextBlob Kütüphanesiyle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=-0.6, subjectivity=0.4)\n",
      "Sentiment(polarity=0.04999999999999999, subjectivity=0.45)\n"
     ]
    }
   ],
   "source": [
    "review1 = 'This computer is really disappointment for me. lack of art'\n",
    "review2 = 'None can find better than this tv game'\n",
    "\n",
    "for review in [review1, review2]:\n",
    "    textBlob = TextBlob(review)\n",
    "    print(textBlob.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[İçindekiler Menüsüne Git](#0.)\n",
    "# <a class='anchor' id ='9.'></a>9. Belirsizlik Giderme  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[İçindekiler Menüsüne Git](#0.)\n",
    "# <a class='anchor' id ='10.'></a>10. Konuşmayı Metne Dönüştürme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please say something\n",
      "Time over, thanks\n",
      "I think you said: Merhabalar beni dinliyor musun Google bilmiyorsan söylediklerimi yaz\n"
     ]
    }
   ],
   "source": [
    "r=sr.Recognizer()\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Please say something\")\n",
    "    audio = r.listen(source)\n",
    "    print(\"Time over, thanks\")\n",
    "try:\n",
    "    print(\"I think you said: \"+r.recognize_google(audio, language='tr'));\n",
    "except:\n",
    "    print('Malesef bir hata oluştu...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[İçindekiler Menüsüne Git](#0.)\n",
    "# <a class='anchor' id ='11.'></a>11. Metni Konuşmaya Dönüştürme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "convert = gTTS(text = 'Bugün yapacağımız işlemleri anlatıyorum: metin sınıflandırma ve duygu analizi',\n",
    "              lang = 'tr', slow = False)\n",
    "convert.save('outputs/yazidan_konusmaya.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[İçindekiler Menüsüne Git](#0.)\n",
    "# <a class='anchor' id ='12.'></a>12. Tercüme Yapma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install goslate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today we'll cover topics in the course include: text classification and sentiment analysis\n"
     ]
    }
   ],
   "source": [
    "import goslate\n",
    "gs = goslate.Goslate()\n",
    "\n",
    "text = 'Bugün derste işleyeceğimiz konular şunlardır: metin sınıflandırma ve duygu analizi'\n",
    "translated_text = gs.translate(text, 'en')\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
